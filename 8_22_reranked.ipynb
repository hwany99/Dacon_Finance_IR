{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/a2024712006/miniconda3/envs/dacon/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import unicodedata\n",
    "\n",
    "import torch\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import fitz  # PyMuPDF\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    pipeline,\n",
    "    BitsAndBytesConfig,\n",
    "    Gemma2ForCausalLM\n",
    ")\n",
    "from accelerate import Accelerator\n",
    "\n",
    "# Langchain 관련\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.schema import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "from langchain_community.vectorstores.utils import DistanceStrategy\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "from langchain_teddynote.retrievers import KiwiBM25Retriever\n",
    "from langchain.retrievers import EnsembleRetriever, MultiQueryRetriever\n",
    "from peft import PeftModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.extract_module import daconCustomExtractor\n",
    "\n",
    "def process_pdf(file_path):\n",
    "    # 1. pdf별 page 추출\n",
    "    doc = fitz.open(file_path)\n",
    "    outpt_dir = \"/home/a2024712006/dacon/extract_image\"\n",
    "    chunk_list = []\n",
    "    for page_number in range(doc.page_count):\n",
    "        page = doc.load_page(page_number)\n",
    "        \n",
    "        #2. page별 테이블 추출 \n",
    "        tables = page.find_tables()\n",
    "        raw_text_list = []\n",
    "        for table in tables:\n",
    "            # 테이블을 감싸는 영역 계산\n",
    "            min_x, min_y = float('inf'), float('inf')\n",
    "            max_x, max_y = float('-inf'), float('-inf')\n",
    "            \n",
    "            for cell in table.cells:\n",
    "                x0, y0, x1, y1 = cell[:4]  # 셀 좌표 추출\n",
    "                min_x = min(min_x, x0)\n",
    "                min_y = min(min_y, y0)\n",
    "                max_x = max(max_x, x1)\n",
    "                max_y = max(max_y, y1)\n",
    "            # 1) 발견된 테이블 영역 \n",
    "            table_rect = fitz.Rect(min_x, min_y, max_x, max_y)\n",
    "            # 2) 발견된 테이블 영역의 테이블 형식 텍스트\n",
    "            table_text = \"\\n\"\n",
    "            for row in table.extract():\n",
    "                table_text += str(row)\n",
    "                table_text += \"\\n\"\n",
    "            # 3) 발견된 테이블 영역의 날 것 텍스트 \n",
    "            clipped_text = page.get_text(\"text\", clip=table_rect)\n",
    "            \n",
    "            # 4) 날 것 텍스트 => 테이블 형식 텍스트로 변환\n",
    "            raw_text_list.append((clipped_text, table_text))\n",
    "            \n",
    "        # 2. page별 이미지 추출        \n",
    "        extractor = daconCustomExtractor(page) \n",
    "        bboxes = extractor.detect_svg_contours(page_number+1, output_dir=outpt_dir, min_svg_gap_dx=25.0, min_svg_gap_dy=25.0, min_w=2.0, min_h=2.0)\n",
    "\n",
    "        # # 텍스트를 chunk로 분할\n",
    "        # splitter = RecursiveCharacterTextSplitter(\n",
    "        #     chunk_size=512,\n",
    "        #     chunk_overlap=32\n",
    "        # )    \n",
    "        # 3. 이미지별 텍스트 추출\n",
    "        for i, bbox in enumerate(bboxes):\n",
    "            x0, y0, x1, y1 = bbox \n",
    "            \n",
    "            full_text = page.get_text(\"text\", clip=fitz.Rect(x0, y0, x1, y1))\n",
    "            for clipped_text, table_text in raw_text_list:\n",
    "                if clipped_text in full_text:\n",
    "                    full_text = full_text.replace(clipped_text, table_text)\n",
    "                    \n",
    "            chunk_list.append(full_text)\n",
    "            # chunk_temp = splitter.split_text(full_text)\n",
    "            # chunk_list.extend(chunk_temp)\n",
    "            \n",
    "    chunks = [Document(page_content=t) for t in chunk_list]\n",
    "    return chunks\n",
    "\n",
    "def create_vector_db(chunks, model_path=\"BAAI/bge-m3\"):\n",
    "    \"\"\"FAISS DB 생성\"\"\"\n",
    "    # 임베딩 모델 설정\n",
    "    model_kwargs = {'device': 'cuda'}\n",
    "    encode_kwargs = {'normalize_embeddings': True}\n",
    "    embeddings = HuggingFaceEmbeddings(\n",
    "        model_name=model_path,\n",
    "        model_kwargs=model_kwargs,\n",
    "        encode_kwargs=encode_kwargs\n",
    "    )\n",
    "    # FAISS DB 생성 및 반환\n",
    "    db = FAISS.from_documents(chunks, embedding=embeddings, distance_strategy = DistanceStrategy.COSINE)\n",
    "    return db\n",
    "\n",
    "def normalize_path(path):\n",
    "    \"\"\"경로 유니코드 정규화\"\"\"\n",
    "    return unicodedata.normalize('NFC', path)\n",
    "\n",
    "def normalize_string(s):\n",
    "    \"\"\"유니코드 정규화\"\"\"\n",
    "    return unicodedata.normalize('NFC', s)\n",
    "\n",
    "def process_pdfs_from_dataframe(df, base_directory):\n",
    "    \"\"\"딕셔너리에 pdf명을 키로해서 DB, retriever 저장\"\"\"\n",
    "    pdf_databases = {}\n",
    "    unique_paths = df['Source_path'].unique()\n",
    "    \n",
    "    for path in tqdm(unique_paths, desc=\"Processing PDFs\"):\n",
    "        # 경로 정규화 및 절대 경로 생성\n",
    "        normalized_path = normalize_path(path)\n",
    "        full_path = os.path.normpath(os.path.join(base_directory, normalized_path.lstrip('./'))) if not os.path.isabs(normalized_path) else normalized_path\n",
    "        \n",
    "        pdf_title = os.path.splitext(os.path.basename(full_path))[0]\n",
    "        print(f\"Processing {pdf_title}...\")\n",
    "        \n",
    "        # PDF 처리 및 벡터 DB 생성\n",
    "        chunks = process_pdf(full_path)\n",
    "        db = create_vector_db(chunks)\n",
    "        kiwi_bm25_retriever = KiwiBM25Retriever.from_documents(chunks)\n",
    "        faiss_retriever = db.as_retriever()\n",
    "        # Retriever 생성\n",
    "        retriever = EnsembleRetriever(\n",
    "            retrievers=[kiwi_bm25_retriever, faiss_retriever],\n",
    "            weights=[0.5, 0.5],\n",
    "            search_type=\"mmr\",\n",
    "        )\n",
    "        \n",
    "        # 결과 저장\n",
    "        pdf_databases[pdf_title] = {\n",
    "                'db': db,\n",
    "                'retriever': retriever\n",
    "        }\n",
    "    return pdf_databases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:   0%|          | 0/9 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 중소벤처기업부_혁신창업사업화자금(융자)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/a2024712006/miniconda3/envs/dacon/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:139: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 0.3.0. An updated version of the class exists in the langchain-huggingface package and should be used instead. To use it run `pip install -U langchain-huggingface` and import as `from langchain_huggingface import HuggingFaceEmbeddings`.\n",
      "  warn_deprecated(\n",
      "Processing PDFs:  11%|█         | 1/9 [00:12<01:43, 12.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 보건복지부_부모급여(영아수당) 지원...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  22%|██▏       | 2/9 [00:24<01:25, 12.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 보건복지부_노인장기요양보험 사업운영...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  33%|███▎      | 3/9 [00:38<01:18, 13.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 산업통상자원부_에너지바우처...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  44%|████▍     | 4/9 [01:11<01:43, 20.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 국토교통부_행복주택출자...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  56%|█████▌    | 5/9 [01:24<01:12, 18.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 「FIS 이슈 & 포커스」 22-4호 《중앙-지방 간 재정조정제도》...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  67%|██████▋   | 6/9 [02:11<01:23, 27.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 「FIS 이슈 & 포커스」 23-2호 《핵심재정사업 성과관리》...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  78%|███████▊  | 7/9 [03:08<01:14, 37.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 「FIS 이슈&포커스」 22-2호 《재정성과관리제도》...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  89%|████████▉ | 8/9 [03:54<00:40, 40.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 「FIS 이슈 & 포커스」(신규) 통권 제1호 《우발부채》...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs: 100%|██████████| 9/9 [04:37<00:00, 30.78s/it]\n"
     ]
    }
   ],
   "source": [
    "base_directory = '/home/a2024712006/dacon' # Your Base Directory\n",
    "df = pd.read_csv('/home/a2024712006/dacon/test.csv')\n",
    "pdf_databases = process_pdfs_from_dataframe(df, base_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "import torch\n",
    "\n",
    "\n",
    "model_id = \"rtzr/ko-gemma-2-9b-it\" \n",
    "\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model_id,\n",
    "    model_kwargs={\"torch_dtype\": torch.bfloat16},\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "pipeline.model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# return: final_context, full_text\n",
    "# final_context: 최종 llm에게 넘길 context\n",
    "# full_doc_text: reranking하기 전 context\n",
    "def llm_reranker(results, question):\n",
    "    context = []\n",
    "    full_doc_text = \"\"\n",
    "    for i in range(len(results)):\n",
    "        if i > 4:\n",
    "            break\n",
    "        context.append(results[i].page_content)\n",
    "        text = f\"문서{i}:\" + \"\\n\\n\" + results[i].page_content\n",
    "        full_doc_text += f\"\\n\\n{text}\\n===\"\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "    정보: \n",
    "        {full_doc_text}\n",
    "\n",
    "    위 정보에서 아래 질문에 대한 답을 유추할 수 있고 가장 유사한 문서를 2개 골라서 문서의 번호만 출력해줘\n",
    "\n",
    "    질문: {question}\n",
    "\n",
    "    \"\"\"\n",
    "    messages = [\n",
    "    {\"role\": \"user\", \"content\": f\"{prompt}\"}\n",
    "    ]\n",
    "\n",
    "    prompt = pipeline.tokenizer.apply_chat_template(\n",
    "        messages, \n",
    "        tokenize=False, \n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "\n",
    "    terminators = [\n",
    "        pipeline.tokenizer.eos_token_id,\n",
    "        pipeline.tokenizer.convert_tokens_to_ids(\"<end_of_turn>\")\n",
    "    ]\n",
    "\n",
    "    outputs = pipeline(\n",
    "        prompt,\n",
    "        max_new_tokens=2048,\n",
    "        eos_token_id=terminators,\n",
    "        do_sample=True,\n",
    "        temperature=0.6,\n",
    "        top_p=0.9,\n",
    "    )\n",
    "    response = outputs[0][\"generated_text\"][len(prompt):]\n",
    "    print(f\"Answer: 0, {response}\\n\")\n",
    "    \n",
    "    reranked_idx_list = [0]\n",
    "    for char in response:\n",
    "        if char.isdigit() and char != \"0\":\n",
    "            idx = int(char)\n",
    "            if idx not in reranked_idx_list and idx < 5:\n",
    "                reranked_idx_list.append(idx)\n",
    "    \n",
    "    final_context = \"\"\n",
    "    for idx in reranked_idx_list:\n",
    "        final_context += f\"\\n{context[idx]}\\n\"\n",
    "    return final_context, full_doc_text\n",
    "\n",
    "# return: response\n",
    "def llm_answer(context, question):\n",
    "    prompt = f\"\"\"\n",
    "    다음 정보를 바탕으로 질문에 답하세요:\n",
    "    {context}\n",
    "\n",
    "    질문: {question}\n",
    "    \n",
    "    주어진 질문에만 답변하세요. 문장으로 답변해주세요. 답변할 때 질문의 주어를 써주세요.\n",
    "    답변:\n",
    "    \"\"\"\n",
    "    messages = [\n",
    "    {\"role\": \"user\", \"content\": f\"{prompt}\"}\n",
    "    ]\n",
    "\n",
    "    prompt = pipeline.tokenizer.apply_chat_template(\n",
    "        messages, \n",
    "        tokenize=False, \n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "\n",
    "    terminators = [\n",
    "        pipeline.tokenizer.eos_token_id,\n",
    "        pipeline.tokenizer.convert_tokens_to_ids(\"<end_of_turn>\")\n",
    "    ]\n",
    "\n",
    "    outputs = pipeline(\n",
    "        prompt,\n",
    "        max_new_tokens=2048,\n",
    "        eos_token_id=terminators,\n",
    "        do_sample=True,\n",
    "        temperature=0.6,\n",
    "        top_p=0.9,\n",
    "    )\n",
    "    response = outputs[0][\"generated_text\"][len(prompt):]\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 정규화된 키로 데이터베이스 검색\n",
    "normalized_keys = {normalize_string(k): v for k, v in pdf_databases.items()}\n",
    "\n",
    "debug_context = {}\n",
    "debug_full_doc_text ={}\n",
    "\n",
    "# DataFrame의 각 행에 대해 처리\n",
    "for _, row in tqdm(df.iterrows(), total=len(df), desc=\"Answering Questions\"):\n",
    "    # 소스 문자열 정규화\n",
    "    source = normalize_string(row['Source'])\n",
    "    question = row['Question']\n",
    "    retriever = normalized_keys[source]['retriever']\n",
    "    results = retriever.get_relevant_documents(question)\n",
    "\n",
    "    context, full_doc_text = llm_reranker(results, question)\n",
    "    debug_context['question'] = context \n",
    "    debug_full_doc_text['question'] = full_doc_text\n",
    "    \n",
    "    print(f\"Question: {question}\")\n",
    "    response = llm_answer(context, question)\n",
    "    print(f\"Answer: {response}\\n\")\n",
    "    \n",
    "    # 결과 저장\n",
    "    results.append({\n",
    "        \"Source\": row['Source'],\n",
    "        \"Source_path\": row['Source_path'],\n",
    "        \"Question\": question,\n",
    "        \"Answer\": response\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 제출용 샘플 파일 로드\n",
    "submit_df = pd.read_csv(\"./sample_submission.csv\")\n",
    "\n",
    "# 생성된 답변을 제출 DataFrame에 추가\n",
    "submit_df['Answer'] = [item['Answer'] for item in results]\n",
    "submit_df['Answer'] = submit_df['Answer'].fillna(\"데이콘\")     # 모델에서 빈 값 (NaN) 생성 시 채점에 오류가 날 수 있음 [ 주의 ]\n",
    "\n",
    "# 결과를 CSV 파일로 저장\n",
    "submit_df.to_csv(\"./submission.csv\", encoding='UTF-8-sig', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dacon",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
